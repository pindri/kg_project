{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd764f-05d9-4c5c-9bed-dd5678160c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d670d2b-f471-43e7-b5d6-7a9a823d2962",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d7e93-6a43-4484-92d4-14f4ed020360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1f2e5-6bbb-4a41-a458-b24151f28ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KG from entities and relations and get them.\n",
    "import data_builder\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import TransE, TransM\n",
    "import numpy as np\n",
    "import utilities\n",
    "\n",
    "%run -i \"data_builder.py\"\n",
    "\n",
    "entities, relations = data_builder.get_entities_and_relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9eee2e-a5ee-442f-9cd4-2cda65e59589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets and indices.\n",
    "\n",
    "import model_utilities\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "entities_index = create_index_dictionary(\"data/kg/entities_index.csv\")\n",
    "relations_index = create_index_dictionary(\"data/kg/relations_index.csv\")\n",
    "\n",
    "kg = model_utilities.MLDataset(\"data/kg/kg.csv\", entities_index, relations_index)\n",
    "train_size = int(0.7 * len(kg))\n",
    "val_size = int(0.1 * len(kg))\n",
    "test_size = len(kg) - train_size - val_size\n",
    "train_kg, test_kg, val_kg = torch.utils.data.random_split(kg, [train_size, test_size, val_size],\n",
    "                                                          generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_kg, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e4092-ee2c-426a-9990-0a87bbfad551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test routine for both TransE and TransM models.\n",
    "\n",
    "def test(model, data_loader, model_name):\n",
    "    precision = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for h, l, t in test_loader:\n",
    "        # print(f\"batch size: {h.size()[0]}\")\n",
    "        h, l, t = (h.to(device), t.to(device), l.to(device))\n",
    "        \n",
    "        if model_name == \"TransM\":\n",
    "            # Compute weights for each relation.\n",
    "            rel_weights = []\n",
    "\n",
    "        batch_size = h.size()[0]\n",
    "\n",
    "        # Build h, l, t for batch.\n",
    "        entities_times_batch = torch.arange(end=len(entities_index), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        h_times_batch = h.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "        l_times_batch = l.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "        t_times_batch = t.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "\n",
    "        # Compute prediction on all head/tail variatios.\n",
    "        head_corrupted_triplets = torch.stack((entities_times_batch, l_times_batch, t_times_batch), dim=2).reshape(-1, 3)\n",
    "        tail_corrupted_triplets = torch.stack((h_times_batch, l_times_batch, entities_times_batch), dim=2).reshape(-1, 3)\n",
    "        \n",
    "        head_corrupted_predictions = model.predict(head_corrupted_triplets).reshape(batch_size, -1)\n",
    "        tail_corrupted_predictions = model.predict(tail_corrupted_triplets).reshape(batch_size, -1)\n",
    "\n",
    "        # Compute precision at 10.\n",
    "        predictions = torch.cat((tail_corrupted_predictions, head_corrupted_predictions), dim=0)\n",
    "        target = torch.cat((t.reshape(-1, 1), h.reshape(-1, 1)))\n",
    "\n",
    "        _, indices = predictions.topk(k=10, largest=False)\n",
    "        precision += torch.where(indices == target, torch.tensor([1], device=device), torch.tensor([0], device=device)).sum().item()\n",
    "        num_examples += predictions.size()[0]\n",
    "\n",
    "    # print(precision/num_examples)\n",
    "        \n",
    "    return(precision/num_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4420d33-5de8-43ca-9cd4-7c6c06d6141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train routine for both TransE and TransM.\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs, summary_writer, model_name=\"TransE\", validation=True):\n",
    "    best_score = 0.0\n",
    "    \n",
    "    print(f\"Training {model.__class__.__name__} model.\")\n",
    "    \n",
    "    training_distances_list = []\n",
    "    corrupted_distances_list = []\n",
    "    loss_list = []\n",
    "    perc_non_zero_loss_samples = []\n",
    "          \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        model.train()\n",
    "\n",
    "        non_zero_loss_samples = 0\n",
    "        sample_count = 0\n",
    "        step = 0\n",
    "\n",
    "        for h, l, t in data_loader:\n",
    "            h, l, t = (h.to(device), t.to(device), l.to(device))\n",
    "            \n",
    "            training_triples = torch.stack((h, l, t), dim=1)\n",
    "            \n",
    "            if model_name == \"TransM\":\n",
    "                # Compute weights for each relation.\n",
    "                rel_weights = []\n",
    "\n",
    "                for rel in l:\n",
    "                    mask = l == rel\n",
    "                    num_rel = mask.sum()\n",
    "                    num_tails = torch.numel(t[mask].unique())\n",
    "                    num_heads = torch.numel(h[mask].unique())\n",
    "                    rel_weights.append(1 / np.log(num_rel/num_tails + num_rel/num_heads))\n",
    "\n",
    "                rel_weights = torch.tensor(rel_weights)\n",
    "            \n",
    "            # Generating corrupted triplets by replacing either head or tail with random entity.\n",
    "            replacement_mask = torch.randint(high=2, size=h.size(), device=device)\n",
    "            corrupted_entities = torch.randint(high=len(entities_index), size=h.size(), device=device)\n",
    "            corrupted_h = torch.where(replacement_mask==0, corrupted_entities, h)\n",
    "            corrupted_t = torch.where(replacement_mask==1, corrupted_entities, t)\n",
    "\n",
    "            corrupted_triples = torch.stack((corrupted_h, l, corrupted_t), dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if model_name == \"TransM\":\n",
    "                loss, training_distances, corrupted_distances = model(training_triples, corrupted_triples, rel_weights)\n",
    "            else:\n",
    "                loss, training_distances, corrupted_distances = model(training_triples, corrupted_triples)\n",
    "            loss.mean().backward()\n",
    "\n",
    "            summary_writer.add_scalar('Loss/train', loss.mean().data.cpu().numpy(), global_step=step)\n",
    "            summary_writer.add_scalar('Distance/training', training_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "            summary_writer.add_scalar('Distance/corrupted', corrupted_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "            \n",
    "            # Appending to list.\n",
    "            loss_list.append(loss.mean().data.cpu().numpy())\n",
    "            training_distances_list.append(training_distances.sum().data.cpu().numpy())\n",
    "            corrupted_distances_list.append(corrupted_distances.sum().data.cpu().numpy())\n",
    "\n",
    "            loss = loss.data.cpu()\n",
    "            non_zero_loss_samples += loss.nonzero().size()[0]\n",
    "            sample_count += loss.size()[0]\n",
    "\n",
    "            optimizer.step()\n",
    "            step+=1\n",
    "            # if step%100 == 0:\n",
    "            #     print(step)\n",
    "\n",
    "        print(f\"\\t train non zero fraction: {non_zero_loss_samples/sample_count}\")\n",
    "        \n",
    "        perc_non_zero_loss_samples.append(non_zero_loss_samples/sample_count)\n",
    "\n",
    "        summary_writer.add_scalar('Metrics/loss_impacting_samples', 10, global_step=epoch)\n",
    "        if validation:\n",
    "            print(f\"\\t validation hit@10: {test(model, test_loader, model_name)}\")\n",
    "        \n",
    "    return loss_list, training_distances_list, corrupted_distances_list, perc_non_zero_loss_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0a283-e787-47ca-bb91-42704a0c3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and parameters.\n",
    "\n",
    "# TransE.\n",
    "model = TransE(num_entities=len(entities), num_relations=len(relations), p=1, k=50, gamma=1.0)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "summary_writer = SummaryWriter()\n",
    "best_score = 0.0\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d7b18-7f2b-42ea-99fe-94c9a35316de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with utilities.codeTimer(f\" {model_name} training\"):\n",
    "    loss, training_distances, corrupted_distances, perc_non_zero_loss = train(model, train_loader, \n",
    "                                                                              optimizer, epochs, \n",
    "                                                                              summary_writer, model_name=\"TransM\",\n",
    "                                                                              validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756dfb9-125e-4014-b963-6c4dc9920394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransM\n",
    "\n",
    "model = TransM(num_entities=len(entities), num_relations=len(relations), p=1, k=50, gamma=1.0)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "summary_writer = SummaryWriter()\n",
    "best_score = 0.0\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e46b0-5c8e-474d-8575-3c0403bea724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, training_distances, corrupted_distances, perc_non_zero_loss = train(model, train_loader, \n",
    "                                                                          # optimizer, epochs, \n",
    "                                                                          # summary_writer, model_name=\"TransM\", \n",
    "                                                                          # validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635339a-41b1-451f-9c65-f5c0261c48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file.\n",
    "\n",
    "results_folder = \"results/\"\n",
    "\n",
    "rows = zip(loss, perc_non_zero_loss)\n",
    "\n",
    "with open(results_folder+f\"{model.__class__.__name__}_train.csv\", \"w\") as f:\n",
    "    f.write(\"loss, perc_non_zero_loss\\n\")\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288dcdb-f551-49c1-b68d-4aa70c8351d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2551d18-3d72-4b07-b714-aa555260bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"TransE\"\n",
    "model_name = \"TransM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98994a5c-a0b8-40d1-8016-08520ed963a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on whole data, i.e., all relations.\n",
    "\n",
    "test_loader = DataLoader(test_kg, batch_size=128)\n",
    "\n",
    "precision_at_10 = 0\n",
    "\n",
    "with utilities.codeTimer(f\"hit@10 {model_name} prediction\"):\n",
    "    precision_at_10 = test(model, test_loader, model_name)\n",
    "    \n",
    "print(precision_at_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebf30e-e16d-4e63-8186-70eb63397f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on feedback specifically.\n",
    "\n",
    "feedback_kg = model_utilities.MLDataset(\"data/kg/kg.csv\", entities_index, relations_index, mask='feedback')\n",
    "train_size = int(0.7 * len(feedback_kg))\n",
    "test_size = len(feedback_kg) - train_size\n",
    "_, test_feedback_kg = torch.utils.data.random_split(feedback_kg, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_loader = DataLoader(test_feedback_kg, batch_size=128)\n",
    "\n",
    "with utilities.codeTimer(f\"hit@10 {model_name} prediction (using only feedback relation)\"):\n",
    "    precision_at_10 = test(model, test_loader, model_name)\n",
    "    \n",
    "print(precision_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75ca22-0862-4f00-afc9-bd489229f3f3",
   "metadata": {},
   "source": [
    "## DP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9568e2-2b5e-4788-9253-6635c619fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "model = TransM(num_entities=len(entities), num_relations=len(relations), p=1, k=50, gamma=1.0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "dp_model, dp_optimizer, dp_train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Privacy accounting.\n",
    "accountant = RDPAccountant()\n",
    "dp_optimizer.attach_step_hook(accountant.get_optimizer_hook_fn(sample_rate=128/len(train_kg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab9491-6686-484d-8d1a-d2f6108ac2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for epoch in range(5):\n",
    "#     print(f\"epoch: {epoch}\")\n",
    "#     dp_model.train()\n",
    "    \n",
    "#     non_zero_loss_samples = 0\n",
    "#     sample_count = 0\n",
    "#     step = 0\n",
    "    \n",
    "#     for h, l, t in dp_train_loader:\n",
    "#         h, l, t = (h.to(device), t.to(device), l.to(device))\n",
    "        \n",
    "#         training_triples = torch.stack((h, l, t), dim=1)\n",
    "        \n",
    "#         # Generating corrupted triplets by replacing either head \n",
    "#         # or tail with random entity.\n",
    "#         replacement_mask = torch.randint(high=2, size=h.size(), device=device)\n",
    "#         corrupted_entities = torch.randint(high=len(entities_index), size=h.size(), device=device)\n",
    "#         corrupted_h = torch.where(replacement_mask==0, corrupted_entities, h)\n",
    "#         corrupted_t = torch.where(replacement_mask==1, corrupted_entities, t)\n",
    "        \n",
    "#         corrupted_triples = torch.stack((corrupted_h, l, corrupted_t), dim=1)\n",
    "        \n",
    "#         dp_optimizer.zero_grad()\n",
    "        \n",
    "#         loss, training_distances, corrupted_distances = dp_model(training_triples, corrupted_triples)\n",
    "#         loss.mean().backward()\n",
    "        \n",
    "#         summary_writer.add_scalar('Loss/train', loss.mean().data.cpu().numpy(), global_step=step)\n",
    "#         summary_writer.add_scalar('Distance/training', training_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "#         summary_writer.add_scalar('Distance/corrupted', corrupted_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "        \n",
    "        \n",
    "#         loss = loss.data.cpu()\n",
    "#         non_zero_loss_samples += loss.nonzero().size()[0]\n",
    "#         sample_count += loss.size()[0]\n",
    "        \n",
    "#         dp_optimizer.step()\n",
    "#         step+=1\n",
    "#         # print(step)\n",
    "#         if step%100 == 0:\n",
    "#             print(step)\n",
    "        \n",
    "#     print(non_zero_loss_samples/sample_count)\n",
    "        \n",
    "#     summary_writer.add_scalar('Metrics/loss_impacting_samples', 10, global_step=epoch)\n",
    "    \n",
    "#     # TODO: add some validation and probably better logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba6d50-7533-454c-9556-0c516ed97445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dp_train(model, data_loader, optimizer, num_epochs, summary_writer, model_name=\"TransE\"):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        model.train()\n",
    "        \n",
    "        loss_list = []\n",
    "        perc_non_zero_loss_samples = []\n",
    "\n",
    "        non_zero_loss_samples = 0\n",
    "        sample_count = 0\n",
    "        step = 0\n",
    "\n",
    "        for h, l, t in train_loader:\n",
    "            h, l, t = (h.to(device), t.to(device), l.to(device))\n",
    "\n",
    "            training_triples = torch.stack((h, l, t), dim=1)\n",
    "            \n",
    "            if model_name == \"TransM\":\n",
    "                # Compute weights for each relation.\n",
    "                rel_weights = []\n",
    "\n",
    "                for rel in l:\n",
    "                    mask = l == rel\n",
    "                    num_rel = mask.sum()\n",
    "                    num_tails = torch.numel(t[mask].unique())\n",
    "                    num_heads = torch.numel(h[mask].unique())\n",
    "                    rel_weights.append(1 / np.log(num_rel/num_tails + num_rel/num_heads))\n",
    "\n",
    "                rel_weights = torch.tensor(rel_weights)\n",
    "\n",
    "            # Generating corrupted triplets by replacing either head \n",
    "            # or tail with random entity.\n",
    "            replacement_mask = torch.randint(high=2, size=h.size(), device=device)\n",
    "            corrupted_entities = torch.randint(high=len(entities_index), size=h.size(), device=device)\n",
    "            corrupted_h = torch.where(replacement_mask==0, corrupted_entities, h)\n",
    "            corrupted_t = torch.where(replacement_mask==1, corrupted_entities, t)\n",
    "\n",
    "            corrupted_triples = torch.stack((corrupted_h, l, corrupted_t), dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_name == \"TransM\":\n",
    "                loss, training_distances, corrupted_distances = model(training_triples, corrupted_triples, rel_weights)\n",
    "            else:\n",
    "                loss, training_distances, corrupted_distances = model(training_triples, corrupted_triples)\n",
    "            loss.mean().backward()\n",
    "\n",
    "            summary_writer.add_scalar('Loss/train', loss.mean().data.cpu().numpy(), global_step=step)\n",
    "            summary_writer.add_scalar('Distance/training', training_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "            summary_writer.add_scalar('Distance/corrupted', corrupted_distances.sum().data.cpu().numpy(), global_step=step)\n",
    "\n",
    "\n",
    "            loss = loss.data.cpu()\n",
    "            non_zero_loss_samples += loss.nonzero().size()[0]\n",
    "            sample_count += loss.size()[0]\n",
    "\n",
    "            optimizer.step()\n",
    "            step+=1\n",
    "            # print(step)\n",
    "            if step%100 == 0:\n",
    "                print(step)\n",
    "\n",
    "        print(f\"\\t train non zero fraction: {non_zero_loss_samples/sample_count}\")\n",
    "        \n",
    "        perc_non_zero_loss_samples.append(non_zero_loss_samples/sample_count)\n",
    "\n",
    "        summary_writer.add_scalar('Metrics/loss_impacting_samples', 10, global_step=epoch)\n",
    "        \n",
    "    return loss_list, perc_non_zero_loss_samples\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac547cc-47be-468d-a628-070ab4fa63a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with utilities.codeTimer(f\"DP-{model_name} training\"):\n",
    "    loss_list, perc_non_zero_loss = dp_train(dp_model, dp_train_loader, dp_optimizer, 10, summary_writer, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32164b-4f7b-4aee-8219-7ec7d238db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant.get_epsilon(delta=1/len(train_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3643b5b-e8e7-4411-8822-80ab9771daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"results/\"\n",
    "\n",
    "rows = zip(loss, perc_non_zero_loss)\n",
    "\n",
    "with open(results_folder+f\"{model.__class__.__name__}_dp_train.csv\", \"w\") as f:\n",
    "    f.write(\"loss, perc_non_zero_loss\\n\")\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ab58d-9182-4553-888a-e24b96a2d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_kg, batch_size=128)\n",
    "with utilities.codeTimer(f\"DP-{model_name} training\"):\n",
    "    test_loader = DataLoader(test_feedback_kg, batch_size=128)\n",
    "\n",
    "    precision = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    dp_model.eval()\n",
    "\n",
    "    for h, l, t in test_loader:\n",
    "        # print(f\"batch size: {h.size()[0]}\")\n",
    "        h, l, t = (h.to(device), t.to(device), l.to(device))\n",
    "\n",
    "        batch_size = h.size()[0]\n",
    "\n",
    "        # Build h, l, t for batch.\n",
    "        entities_times_batch = torch.arange(end=len(entities_index), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        h_times_batch = h.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "        l_times_batch = l.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "        t_times_batch = t.reshape(-1, 1).repeat(1, entities_times_batch.size()[1])\n",
    "\n",
    "        # Compute prediction on all head/tail variatios.\n",
    "        head_corrupted_triplets = torch.stack((entities_times_batch, l_times_batch, t_times_batch), dim=2).reshape(-1, 3)\n",
    "        tail_corrupted_triplets = torch.stack((h_times_batch, l_times_batch, entities_times_batch), dim=2).reshape(-1, 3)\n",
    "\n",
    "        # Note, the dp wrapper does not allow to compute predict(), so this is a workaround to get distances using forward().\n",
    "        _, _, head_corrupted_predictions = dp_model(head_corrupted_triplets, head_corrupted_triplets)\n",
    "        _, _, tail_corrupted_predictions = dp_model(tail_corrupted_triplets, tail_corrupted_triplets)\n",
    "\n",
    "        # Compute precision at 10.\n",
    "        predictions = torch.cat((tail_corrupted_predictions, head_corrupted_predictions), dim=0)\n",
    "        target = torch.cat((t.reshape(-1, 1), h.reshape(-1, 1)))\n",
    "\n",
    "        _, indices = predictions.topk(k=10, largest=False)\n",
    "        precision += torch.where(indices == target, torch.tensor([1], device=device), torch.tensor([0], device=device)).sum().item()\n",
    "        num_examples += predictions.size()[0]\n",
    "\n",
    "\n",
    "    print(precision/num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2235e39-e5f5-4286-b2de-29b8b5563d47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a84813-4309-42c4-be51-7025922a6d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
